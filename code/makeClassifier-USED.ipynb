{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to read...\n",
      "...read\n",
      "['Gian Iphone' 'Portable ECG moniter 1' 'Bodimetrics Performance monitor'\n",
      " 'Apple Watch' 'Eko Stethescope' 'iHealth Blood Pressure'\n",
      " 'Portable ECG moniter 2' 'Fever Sense' 'Portable ECG' 'iHealth gluco'\n",
      " 'Omron V10' 'Pyle Health' 'Portable_ECG' 'Eko Sthethoscope']\n",
      "F1 Obtained from preliminary testing is 0.9293496689893044\n",
      "dict_keys(['fit_time', 'score_time', 'test_acc', 'train_acc', 'test_f1_macro', 'train_f1_macro'])\n",
      "Accuracy Obtained from preliminary testing is 0.9569628936873604\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#THaW Project\n",
    "#This program is meant to establish the accuracy of predicting device name from the data collected by MSU\n",
    "#6/12/2019\n",
    "#Code Written By: Manzi Bryan with a lot of help from https://www.kaggle.com/nageshnaik/iris-dataset-classfication-using-naive-bayes\n",
    "# This code should be submitted for review \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import *\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from path import Path\n",
    "import warnings\n",
    "import pickle\n",
    "import operator\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "target = 'Model'\n",
    "\n",
    "\n",
    "# This method is meant to build a Random Forest Classifier given a dataframe with devices as targets\n",
    "# The dontTrainOn parameter is meant to tell the classifier to leave one tenth of one device out of training\n",
    "\n",
    "# This method is meant to build a Random Forest Classifier given a dataframe with devices as targets\n",
    "\n",
    "def makeClassifier(df, functionName, clf, labels, f1Dataframe = None, i= None,):\n",
    "    \n",
    "    inTestingMode = not (f1Dataframe == None and i == None)\n",
    "    devices = df[target].unique()\n",
    "    print(devices)\n",
    "    copydf = df.copy()\n",
    "    \n",
    "    copydf[target].replace(devices, range(0, len(devices)), inplace=True)\n",
    "        \n",
    "    Y = copydf[target].tolist()\n",
    "    \n",
    "#     print(\"There are \" + str(len(devices)) + \" devices\")\n",
    "    \n",
    "    #Remove labelling columns from the index\n",
    "    copydf = copydf.drop(columns= labels)\n",
    "    X = copydf.values\n",
    "    \n",
    "    \n",
    "    #One tenth of the data as test\n",
    "    validation_size = 0.1\n",
    "    \n",
    "    seed = 7\n",
    "    \n",
    "    X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, \n",
    "                                                    test_size=validation_size, random_state=seed)\n",
    "    \n",
    "#     scoring = {'acc': 'accuracy',\n",
    "#            'prec_macro': 'precision_macro',\n",
    "#            'rec_macro': 'recall_macro',\n",
    "#             'f1_macro' : 'f1_macro'}\n",
    "    scoring = {\n",
    "            'acc': 'accuracy',\n",
    "            'f1_macro' : 'f1_macro'}\n",
    "\n",
    "    \n",
    "    #Fitting the training set\n",
    "    clf.fit(X_train, Y_train) \n",
    "    \n",
    "   \n",
    "    #Model Performance\n",
    "    #setting performance parameters\n",
    "    kfold = model_selection.StratifiedKFold(n_splits=10, random_state=seed) #same number of samples from each \n",
    "\n",
    "    #calling the cross validation function\n",
    "    \n",
    "    cv_results = cross_validate(clf, X_train, Y_train, cv=kfold, scoring=scoring, return_train_score=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    if not inTestingMode:\n",
    "        filename = functionName  + 'Model.sav' # Save Classifier\n",
    "        pickle.dump(clf, open(filename, 'wb'))\n",
    "        print(\"F1 Obtained from preliminary testing is \" + str(cv_results['test_f1_macro'].mean()))\n",
    "        print(cv_results.keys())\n",
    "        print(\"Accuracy Obtained from preliminary testing is \" + str(cv_results['test_acc'].mean()))\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        f1Dataframe['Classifier'][i] = functionName\n",
    "        f1Dataframe['f1'][i] = str(cv_results['test_f1_macro'].mean())\n",
    "    \n",
    "    return clf\n",
    "    \n",
    "def makeOneClassifier(df, labels):\n",
    "    n_est = 15\n",
    "    depth = 15\n",
    "    name = 'RandomForest d=' + str(depth) + ' n_est=' + str(n_est)\n",
    "    classifier = RandomForestClassifier(n_estimators=n_est, max_depth=depth)\n",
    "\n",
    "    devices = df[target].unique()\n",
    "    df = pd.read_excel(path)\n",
    "\n",
    "    df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)#Remove unnecessary columns from the index\n",
    "#         df.drop(df.columns[df.columns.str.contains('Bursts',case = False)],axis = 1, inplace = True)#Remove unnecessary columns from \n",
    "    makeClassifier(df, name, classifier, labels)\n",
    "    return devices\n",
    "    \n",
    "def makeManyClassifiers(df, labels):\n",
    "    \n",
    "    functions = {}\n",
    "    f1Dataframe = pd.DataFrame(index = range(0, 10000), columns=['Classifier', 'f1'])\n",
    "    # Grid search from 1 to 100\n",
    "    for depthDivideBy5 in range(1, 21):\n",
    "        for n_estDivideBy5 in range(1, 21):\n",
    "            depth = depthDivideBy5 * 5\n",
    "            n_est = n_estDivideBy5 * 5\n",
    "            functions['RandomForest d=' + str(depth) + ' n_est=' + str(n_est)] = RandomForestClassifier(n_estimators=n_est, max_depth=depth)\n",
    "    \n",
    "#     functions['RandomForest d=14 n_est=25'] = RandomForestClassifier(n_estimators=14, max_depth=25)\n",
    "#     functions['RandomForest d=22 n_est=55'] = RandomForestClassifier(n_estimators=22, max_depth=55)\n",
    "    devices = df[target].unique()\n",
    "    i = 0\n",
    "    for functionName in functions:\n",
    "        df = pd.read_excel(path)\n",
    "        \n",
    "        df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)#Remove unnecessary columns from the index\n",
    "#         df.drop(df.columns[df.columns.str.contains('Bursts',case = False)],axis = 1, inplace = True)#Remove unnecessary columns from \n",
    "        makeClassifier(df, functionName, functions[functionName], labels, f1Dataframe, i)\n",
    "        i += 1\n",
    "    f1Dataframe.to_excel('./BestClassifiers.xlsx')\n",
    "    return devices\n",
    "    \n",
    "\n",
    "def getNumerical(devices, y):\n",
    "    i = 0\n",
    "    \n",
    "    for device in devices:\n",
    "        if device == y[0]:\n",
    "            return [i] * len(y)\n",
    "        i += 1\n",
    "\n",
    "def makePrediction(model, unseen, devices, numReadings, deviceName, labels):\n",
    "    loadedModel = pickle.load(open(model, 'rb'))\n",
    "    unseen = unseen.reindex(sorted(unseen.columns), axis=1)\n",
    "    \n",
    "    unseen.drop(unseen.columns[unseen.columns.str.contains('Unname',case = False)],axis = 1, inplace = True)\n",
    "    \n",
    "    y = unseen[target].tolist() #unseen should only contain one device, hence y is a list of the same device repeated\n",
    "    \n",
    "    \n",
    "    unseen = unseen.drop(columns= labels)\n",
    "    predictions = loadedModel.predict(unseen)\n",
    "    \n",
    "    predictions_proba = loadedModel.predict_proba(unseen)\n",
    "    i = 0\n",
    "    other = 0\n",
    "    confidences = []\n",
    "    for i in range(0, len(predictions_proba)):\n",
    "        confidences.append(round(predictions_proba[i].max(), 3))\n",
    "        \n",
    "    print(len(predictions_proba))\n",
    "    confidences.sort()\n",
    "    \n",
    "    \n",
    "def makeHisto(confidences, deviceName):\n",
    "    # Make Histograms\n",
    "    bins = np.linspace(confidences[0], confidences[-1])\n",
    "    \n",
    "    num_bins = 100 # <-- Change here - Specify total number of bins for histogram\n",
    "    plt.hist(confidences, bins=np.linspace(np.min(confidences), np.max(confidences), num=num_bins)) #<-- Change here.  Note the use of ravel.\n",
    "    plt.savefig(deviceName + ' Histogram')\n",
    "    plt.cla()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    path=r'C:\\Users\\brnma\\train.xlsx'\n",
    "    \n",
    "    print(\"About to read...\")\n",
    "    df = pd.read_excel(path) \n",
    "    print(\"...read\")\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    \n",
    "    labels = ['Device','Model','App','Distance']\n",
    "#     makeManyClassifiers(df, labels) \n",
    "    makeOneClassifier(df, labels)\n",
    "    print(\"Done\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
